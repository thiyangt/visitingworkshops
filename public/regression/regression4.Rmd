---
title: "STA 506 2.0 Linear Regression Analysis"
subtitle: "Lecture 4: Interpretation of Regression Outputs"
author: "Dr Thiyanga S. Talagala"
date: "2020-09-19"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - default
      - default-fonts
      - duke-blue
      - hygge-duke
      - libs/cc-fonts.css
      - libs/figure-captions.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```


## Recapping what we know

**True linear regression model/ Population regression equation**

$$\mu_{Y|X=x} = \beta_0 + \beta_1x $$

- how the mean of $Y$ changes for given values of $X$.

- We can also write the equation in terms of the observed values of $Y$, rather than the mean.  Because the individual observations for any given value of $X$ vary randomly about the mean of $Y|X$, we need to account for this random variation, or error, in the regression equation.  Sample regression line

$$y_i = \beta_0 + \beta_1x_i + \epsilon_i, \text{ i = 1, 2, 3...n}$$

**Fitted regression line**

$$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1x$$


---
# Revisiting the fitted regression model

## Fitted model

```{r, comment=NA, warning=FALSE}
library(alr3) # to load the dataset
model1 <- lm(Dheight ~ Mheight, data=heights)
model1
```

---
## Model summary

```{r, comment=NA, warning=FALSE}
summary(model1)
```

---
# $R^2$ = 24.08%

- Maybe you have one or more omitted variables. It is important to consider other factors that might influence the daughter's height:

    - Father's height

    - Physical activities performed by the daughter

    - Food nutrition, etc.

- Maybe the functional form of the regression form is incorrect (so you have to add some quadratic, or cubic terms...). At the same time a transformation can be an alternative (if appropriate).

- Maybe could be the effect of a group of outlier (maybe not one...).

---


- A large $R^2$ does not necessarily imply that the regression model will be ab accurate predictor.

- $R^2$ does not measure the appropriateness of the linear model.

- $R^2$ will often large even though $Y$ and $X$ are nonlinearly related. 

---
```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE,  fig.cap="Scatterplot of  price vs carat in diamonds (Pearson's correlation coefficient = 0.783)"}
set.seed(55)
library(tidyverse)
df <- tibble(x_var = runif(100, min = 0, max = 25)
             ,y_var = log2(x_var) + rnorm(100)
             )
ggplot(data=df, aes(x=x_var, y=y_var)) + 
  geom_point() + labs(x="carat", y="price")
cor(df$x_var, df$y_var)
```
---

```{r, echo=FALSE, comment=NA}
colnames(df) <- c("price", "carat")
mod <- lm(carat ~ price, data=df)
summary(mod)
```


- $R^2$ is large eventhough the linear approximation is poor.

> In general a good policy is to observe the scatterplot of your data 

---
class:  duke-orange, center, middle

![](crystal.jpg)

 Y - Crystal size
 
 X - Number of hours it takes crystal to grow

---
## Data set

```{r, comment=NA}
crystaldata <- data.frame(Size = c(0.08, 0.07,  1.12, 2.01, 4.43, 
                                   4.98, 4.92, 7.18, 
                                   5.57, 8.40, 8.81, 10.81, 11.61, 10.12, 13.12, 15.04, 0.075, 0.075,  1.13, 2.02, 4.45, 
                                   4.99, 4.95, 7.28, 
                                   5.67, 8.40, 8.51, 10.82, 11.62, 10.13, 13.22, 15.03),
  Time= c(2, 2, 4, 4,6, 8, 10, 12, 14, 16, 18, 20, 20, 24, 26, 28, 2, 2, 4, 4,6, 8, 10, 12, 14, 16, 18, 20, 20, 24, 26, 28))
crystaldata
```
---
## scatterplot

```{r, comment=NA, message=FALSE, fig.height=5, fig.width=5}
ggplot(crystaldata, aes(x=Time, y = Size)) + geom_point()
cor(crystaldata$Time, crystaldata$Size)
```
---

## Fit a simple linear regression model

```{r, comment=NA, message=FALSE}
mod1 <- lm(Size ~ Time, data=crystaldata)
mod1
```

---
## Visualise the model

```{r, comment=NA, fig.height=5}
ggplot(crystaldata, aes(y=Size, x=Time))+geom_point(alpha=0.5) +
  geom_abline(intercept = -0.21, slope = 0.52, colour="forestgreen", lwd=2) +
            theme(aspect.ratio = 1)

```

---
## Questions

1. How well does this equation fit the data?

2. Are any of the bsic assumptions violated?


---


```{r, comment=NA}
summary(mod1)

```


---
## Coefficient of determination


```{r, comment=NA, echo=FALSE}
summary(mod1)

```

- 94.5% of the variability in size is accunted by the regression model. 

or

- 94.5% of the variation in size  is explained by the number of hours it takes crystal to grow.


---
## Are any of the basic assumptions violated?

.pull-left[

```{r , comment=NA}
mod1

```

]

.pull-right[
- Fitted value and Residuals

```{r, comment=NA}
crystaldata
```

]

---

## Compute Residuals and Fitted values

```{r, comment=NA, message=FALSE, comment=NA}
library(broom)
mod1_fitresid <- augment(mod1)
mod1_fitresid

```

---

### 1) The relationship between the response $Y$ and the regressors is linear, at least approximately.

Plot of residuals vs fitted values and Plot of residuals vs predictor 


.pull-left[

Residuals vs Fitted


```{r, comment=NA, echo=FALSE}
ggplot(mod1_fitresid, 
  aes(x = .fitted, y = .resid))+
  geom_point() +
  geom_hline(yintercept = 0, col = "blue", size = 1)

```
]


.pull-right[

Residuals vs Time


```{r comment=NA, echo=FALSE}
ggplot(mod1_fitresid, 
  aes(x = Time, y = .resid))+
  geom_point() +
  geom_hline(yintercept = 0, col = "blue", size = 1)

```
]

---

## 2) The error term $\epsilon$ has zero mean.

## 3) The error term $\epsilon$ has constant variance $\sigma^2$.

Residuals vs Fitted

```{r, comment=NA, echo=FALSE, fig.height=5, fig.width=5}
ggplot(mod1_fitresid, 
  aes(x = .fitted, y = .resid))+
  geom_point(alpha=0.5) +
  geom_hline(yintercept = 0, col = "blue", size = 1)

```


---

## 4) The error are uncorrelated.

---
## 5) The errors are normally distributed.


.pull-left[
```{r, comment=NA, message=FALSE, fig.height=4.5}
ggplot(mod1_fitresid, 
       aes(x=.resid))+
  geom_histogram(colour="white")+ggtitle("Distribution of Residuals")
```
]

.pull-right[
```{r, comment=NA, message=FALSE, fig.height=4.5}
ggplot(mod1_fitresid, 
       aes(sample=.resid))+
  stat_qq() + stat_qq_line()+labs(x="Theoretical Quantiles", y="Sample Quantiles")
```

]

```{r, comment=NA, message=FALSE, fig.height=4}
shapiro.test(mod1_fitresid$.resid)
```


---
## Interpretations

**Slope:**



**Intercept:**

Sometimes $X$ can never realistically take the value of zero, then the intercept is meaningless.

---

```{r, comment=NA}
mod1
```


---

## Hypothesis testing - Intercept

$H_0: \beta_1=0$

$H_1: \beta_1 \neq 0$

```{r, comment=NA}
summary(mod1)
```

---
## Situations where $H_0: \beta_1=0$ is not rejected


---
## Situations where $H_0: \beta_1=0$ is rejected

---

## Analysis of variance - ANOVA

- 

---
## Next Lecture

> Multiple linear regression
---
class: center, middle



Acknowledgement

Introduction to Linear Regression Analysis, Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining

All rights reserved by 

[Dr. Thiyanga S. Talagala](https://thiyanga.netlify.app/) 

